[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Manejo de Inventario",
    "section": "",
    "text": "1 Introducción\nDentro del área del Control Estocástico, una de los problemas más conocidos son los problemas de inventario. Donde se presenta una bodega con capacidad máxima \\(K\\). Cada etapa se extrae una cantidad de mercancía, la que denotaremos como la demanda \\(D_t\\), y se solicita una cantidad del producto \\(a_t\\), obteniendo finalmente el nivel de inventario \\(X_t\\) (ver Sutton and Barto (2018)). En general se busca minimizar los costos de la bodega (costos por almacenamiento, costos por pérdida, entre otros).\n\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Cambridge, MA, USA: A Bradford Book."
  },
  {
    "objectID": "pdm.html",
    "href": "pdm.html",
    "title": "2  Formulación del Proceso de Decisión de Markov.",
    "section": "",
    "text": "Para nuestro problema consideraremos un supermercado, centrado en uno de sus pasillos. Suponiendo que en un pasillo se almacena un solo tipo de producto. Definiremos a \\(K\\) la cantidad máxima de producto en el pasillo, \\(X_t\\) a la cantidad del producto disponible para la venta (o la cantidad de producto en el pasillo). Nuestra demanda, o producto solicitado, será denotado por \\(D_t\\) y se considerará una colección de v.a i.i.d. Finalmente, la cantidad recolocada en el pasillo, o producto pedido, será denotada por \\(a_t\\). Entonces, nuestro conjunto de estados \\(\\mathcal{S}\\) está dado por el siguiente conjunto.\n\\[\n    \\mathcal{S} = \\{s\\in \\mathbb{Z}^+:0\\leq s\\leq K\\}.\n\\tag{2.1}\\]\nNuestro conjunto de acciones \\(\\mathcal{A} = \\mathcal{S} = \\mathbb{Z}^+\\), y para \\(x\\in\\mathcal{S}\\) nuestro conjunto de acciones admisibles esta dado por\n\\[\n    \\mathcal{A}(x)=\\{a\\in \\mathcal{A}:0 \\leq a\\leq K - x\\}.\n\\]"
  },
  {
    "objectID": "dynamics.html",
    "href": "dynamics.html",
    "title": "3  Dinámica del Modelo.",
    "section": "",
    "text": "Recordando la fórmula para nuestro modelo.\n\\[\n    X_{t+1} = f(X_t, a_t), f:\\mathcal{S}\\times \\mathcal{A}\\to \\mathcal{S}.\n\\]\nEntonces el modelo que usaremos esta dado por \\[\n    X_{t+1} = (X_t+a_t-\\eta X_{t} - D_{t+1})^+,\n\\tag{3.1}\\]\ndonde \\(a_t\\) es la cantidad de producto recolodado al final del dia \\(t\\), \\(\\eta\\) es el factor descomposición, \\(D_t\\) es la demanda del prodcucto en la dia \\(t\\) y \\((\\cdot)^+ = \\max\\{\\cdot, 0\\}\\)."
  },
  {
    "objectID": "description.html",
    "href": "description.html",
    "title": "4  Descripción y Justificación del Modelo.",
    "section": "",
    "text": "El modelo Equation 3.1 pretende responder a la pregunta que denota el modelo ¿Cuánto producto tendré disponible al dia siguiente?. Lo anterior menciona que nuestras etapas \\(t\\in \\mathcal{T} = \\{t\\in \\mathbb{Z}^+: t\\leq T,T\\in \\mathbb{N}\\}\\) representaran los dias dentro de un periodo \\(T\\), \\(t\\) hace referencia al dia actual, y \\(t+1\\) al dia siguiente. Entonces el modelo general esta dado por\n\\[\n    X_{t+1} = (\\text{Today} + \\text{In}_t - \\text{Out}_{t})^+.\n\\]\nEsto es, la parte positiva del producto que hay “hoy”, es decir, \\(X_t\\). A eso le agregaremos el producto que entrará hoy al final del dia, en nuestro modelo solo habrá ingreso de producto mediante solicitud (En este caso no consideramos un almacenimiento dentro del supermercado), entonces \\(\\text{In}_t\\) esta dado por nuestras acciones \\(\\text{In}_t = a_t\\).\nLa parte que saldrá consta de dos elementos. En general consideramos la cantidad de producto que se compró en el dia \\(t\\). Sin embargo, desconocemos la cantidad requerida, haciendo referencia al dia siguiente. Por lo tanto la demanda está representada por \\(D_{t+1}\\), la cantidad de producto requerida al dia siguiente. En nuestro modelo también consideramos la salida de producto por considerarse producto no apto para la venta. Entonces\n\\[\n    \\text{Out}_t = D_{t} + N_t(X_t).\n\\]\nBajo de la suposición que todos los productos poseen el mismo tiempo de vida con periodos de vida distintos supondremos que cada dia, al final, se retira un factor con respecto a la cantidad actual de producto. \\[\n    N_t = \\eta X_t\n\\]\n\\[\n    \\text{Out}_t = D_{t} + \\eta X_t\n\\]\nFinalmente, nos queda definir la función de costo, en nuestro modelo será la ganancia. Al considerar un periodo finito tenemos que la ganancia total \\(G\\) esta dada por\n\\[\n    G(x_0, \\pi) = \\sum_{t=0}^{T} R_t,X_0 = x_0, X_{t+1} = f(X_t, a_t), R_t = R(x_t,a_t,\\xi_t)\n\\]\ndonde \\(\\pi\\) es una politica, \\(\\pi = (a_0,a_1, \\ldots, a_{N-1})\\). y \\(R_t\\) es la ganancia por etapa, en nuestro caso \\[\n    R(x,a,\\xi) = P_V \\min\\{x + a, \\xi\\} - P_C a - P_C \\min{\\xi - x - a, 0}.\n\\]\nAl intervenir una variable aleatoria, entonces nos interesaría el valor esperado.\n\\[\n    \\mathcal{G}(x) = E_\\pi[G\\mid X_t = x].\n\\]"
  },
  {
    "objectID": "actions.html",
    "href": "actions.html",
    "title": "5  Justificación de las acciones.",
    "section": "",
    "text": "Ya comentamos que nuestras acciones, serán la cantidad de producto que vamos a solicitar. Entonces nuestras acciones serán números enteros las acciones serán ejecutadas de forma instantea. El conjunto de acciones está dado por \\[\n    \\mathcal{A} = \\{z\\in \\mathbb{Z}^+ : z\\leq K \\}.\n\\]\ny para cada \\(x\\in\\mathcal{S}\\), obtenemos el conjunto de acciones admisibles.\n\\[\n    \\mathcal{A}(x) = \\{z\\in \\mathbb{Z}^+ : z\\leq K - x \\}.\n\\]"
  },
  {
    "objectID": "implementation.html",
    "href": "implementation.html",
    "title": "6  Implementación.",
    "section": "",
    "text": "Finalmente, mediante la ecuación de Bellman.\n\\[\n    v(x) = \\min_{a\\in \\mathcal{A}(x)} \\{E\\left[R(x,a,\\xi) + \\beta v(f(x,a,\\xi))\\right]\\}\n\\tag{6.1}\\]\ny las siguientes condiciones iniciales.\n\nX_0 = 60.\nP_V = 30, P_C = 60.\neta = 0.1.\n\ny considerando una demanda \\(\\xi_t\\) con \\(\\xi_t \\sim \\text{Geo}(p)\\), con \\(p = 0.05\\). Ejecutamos el siguiente código para resolverlo.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.interpolate import CubicSpline\n\n\nRV_P = 0.05  # Parametro Demanda Aleatoria\n\nREW_SALE = 20  # Precio de Venta\nREW_COST = 10  # Costo Unitario\nDYN_ETA = 0.1  # Factor de Pérdida\nM_K = 100  # Capacidad máxima del inventario\n\nX_0 = 60  # Inventario Inicial\nPERIOD = 100 # Periodo Inicial\n\n\ndef demmand(p):\n    \"\"\"\n        Define the Random demmand in the inventory model.\n        Params:\n            p (float): Probabiliy parameter.\n        Return:\n            d (float): Random value\n    \"\"\"\n    d = np.random.geometric(p)\n    return d\n\n\ndef admisible_actions(x, k):\n    \"\"\"\n        Calculate the admisible actions for each state X\n        Params:\n            x (int): state value.\n            k (int): max inventory capacity.\n        Returns:\n            v (list): admisible action set by value x\n    \"\"\" \n    v = list(range(k-x+1))\n    return v\n\n\ndef reward_function(x, a, d, p_v, p_c):\n    \"\"\"\n        Reward Function: Alternative Cost Function\n        Params:\n            x (int): state value\n            a (int): action value\n            d (int): demmand value\n            p_v (float): sale price\n            p_c (float): unitary cost\n        Return:\n            r (float): Reward by x,a and d with parameters p_v and p_c\n    \"\"\"\n    lost_state = 1  # Activate or Deactivate additional cost.\n    lost_cost = max(d - x - a, 0)\n    c = a + lost_state * lost_cost\n    r = p_v * min(x + a, d) - p_c * c\n    return r\n\n\ndef dynamic(x, a, d, eta):\n    \"\"\"\n        Calcuate the next state\n        Params:\n            x (int): state value\n            a (int): action value\n            d (int): demmand value\n            eta (float): loss parameter\n        Return:\n            xp1 (int): next state\n    \"\"\"\n    xp1 = x + a - d - np.floor(eta * x)\n    xp1 = max(xp1, 0)\n    return xp1\n\ndef expected_reward(x, a, d_v):\n    \"\"\"\n        Calculate the Expected Reward\n        x (int): state value\n        a (int): action value\n        d_v (list): random demmand vector\n        Return:\n            data (list) reward demmand function (next to calculate mean)\n    \"\"\"  \n    p_v = REW_SALE\n    p_c = REW_COST\n    data = [reward_function(x, a, d, p_v, p_c) for d in d_v]\n    return data\n\n\ndef expected_vf(vf, x_0, a, d_v):\n    \"\"\"\n        Calculate the function v\n        Params:\n            vf (function): Interpolation\n            x_0 (int): state value\n            a (int) : action value\n            d_v (list): demmand random sample\n        Return:\n        vf_d (int): Expected vf function\n    \"\"\"\n    vf_d = [float(vf(dynamic(x_0, a, d, DYN_ETA))) for d in d_v]\n    vf_d = int(np.array(vf_d).mean())\n    return vf_d\n\nSTATES_SET = list(range(M_K + 1))\ndef evol_vk(vk, hk, beta, der):\n        \"\"\"\n        Calculus of v_k\n        Params:\n            vk (list): iterative function vk\n            hk (list): iterative function hk\n            beta (float):\n            der (list):\n        Return:\n            vk (list): next function iterative hk\n            hk (list): next function iterative vk\n    \"\"\"\n    for u, x_k in enumerate(STATES_SET):\n        a_x = admisible_actions(x_k, M_K)\n    #     print(\"Admisible Actions\",a_x)\n        cost_ax = []\n        expec_ax = []\n        v_function = CubicSpline(STATES_SET, vk)\n        for action in a_x:\n            cost = [reward_function(x_k, action, d, REW_SALE, REW_COST) for d in der]\n            cost = int(np.array(cost).mean())\n            cost_ax.append(cost)\n            e_vf = expected_vf(v_function, x_k, action, der)\n            expec_ax.append(e_vf)\n        v_cost = np.array(cost_ax)\n        v_vf = np.array(expec_ax)\n        v_sum = v_cost + beta * v_vf\n        v_max = v_sum.max()\n        vk[u] = v_max\n        hk[u] = v_sum.argmax()\n    return vk, hk\n\n\nfinal_df = pd.DataFrame()\n\nfor ks in range(10):\n    print(ks)\n    demmand_vector = [demmand(RV_P) for _ in range(200)]\n    v_0 = list(STATES_SET)  # v_0(x) = x\n    h_0 = np.zeros(len(v_0))\n\n    v_mem = [v_0]\n    h_mem = [h_0]\n\n    BPAR= 0.9\n    for kt in range(25):\n        v_1, h_1 = evol_vk(v_0, h_0, beta= BPAR, der = demmand_vector)\n        v_mem.append(v_1)\n        h_mem.append(h_1)\n   #      print(f\"{kt+1} - {kt}\",np.array(v_1) - np.array(v_mem[kt]))\n        v_0 = v_1.copy()\n        h_0 = h_1.copy()\n\n    test_eval = CubicSpline(STATES_SET, h_mem[-1])\n\n    df = pd.DataFrame(index = list(range(PERIOD)),\n                    columns=[\"state\",\"action\",\"demmand\",\"r\"])\n\n    x_hist = [X_0]\n    a_hist = []\n    d_hist = []\n    for q in range(PERIOD):\n        a = int(test_eval(x_hist[q]))\n        d = demmand(RV_P)\n        r = reward_function(x_hist[q], a, d, REW_SALE, REW_COST)\n        df.loc[q] = [x_hist[q], a, d, r]\n        xp1 = dynamic(x_hist[q], a, d, DYN_ETA)\n        x_hist.append(xp1)\n        a_hist.append(a)\n        d_hist.append(d)\n    df['SAMPLE'] = ks\n    if ks == 0:\n        final_df = df\n    else:\n        final_df = pd.concat([final_df, df])\n\n\n# Save data in DataFrame\nfinal_df.to_excel(f\"S_{PERIOD}_({M_K, DYN_ETA})_{RV_P}_({REW_SALE, REW_COST}).xlsx\")\nCódigo para la Visualización.\n# Librerias Requeridas\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Leemos los datoa generados.\ndata = pd.read_excel(\"S_100_((100, 0.1))_0.05_((20, 10)).xlsx\", index_col=0)\nprint(data)\n\n# Hacemos las graficas solcitadas.\nsamples = data['SAMPLE'].unique()\nfigure_x, ax0 = plt.subplots(figsize = (6, 4))  # Grafica de Nivel de Inventario.\nfigure_r, ax1 = plt.subplots(figsize = (6, 4))  # Grafica de la Recompensa por etapa\nfigure_acr, ax2 = plt.subplots(figsize = (6, 4))  # Gráfica de la Recompensa acumulada. \nfigure_act, ax3 = plt.subplots(figsize = (6, 4))  # Gráfica de las politicas. \nx = 0\nr = 0\nacr = 0\nfor w, s in enumerate(samples[:3]):\n    data_s = data[data['SAMPLE'] == s]\n    ax0.plot(data_s['state'], lw = 0.7, label = f\"l_{s}\")\n    ax0.set_xlabel(\"Stages\")\n    ax0.set_ylabel(\"Level\")\n    ax1.plot(data_s['r'], lw = 0.7, label = f\"l_{s}\")\n    ax1.set_ylabel(\"Reward\")\n    ax1.set_xlabel(\"Stages\")\n    ax2.plot(data_s['r'].cumsum(), lw = 0.7, label = f\"l_{s}\")\n    ax2.set_xlabel(\"Stages\")\n    ax2.set_ylabel(\"Acc. Reward\")\n    ax3.plot(data_s['action'], lw = 0.7, label = f\"l_{s}\")\n    ax3.set_xlabel(\"Stages\")\n    ax3.set_ylabel(\"Action\")\n    if w == 0:\n        x = data_s['state']\n        r = data_s['r']\n        acr = data_s['r'].cumsum()\n    else:\n        x += data_s['state']\n        r += data_s['r']\n        acr += data_s['r'].cumsum()\nax0.plot(x / 3, lw = 0.7, color = \"black\", label = \"x mean\")    \nax1.plot(r / 3, lw = 0.7, color = \"black\", label = \"r mean\")\nax2.plot(acr / 3, lw = 0.7, color = \"black\", label = \"acc. r mean\")\nfigure_x.legend()\nfigure_x.tight_layout()\nfigure_x.savefig(\"Data_x.png\")\nfigure_r.legend()\nfigure_r.tight_layout()\nfigure_r.savefig(\"Data_r.png\")\nfigure_acr.legend(loc = \"upper center\")\nfigure_acr.tight_layout()\nfigure_acr.savefig(\"Data_acr.png\")\nfigure_act.legend()\nfigure_act.tight_layout()\nfigure_act.savefig(\"Data_act.png\")\nplt.show()"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement\nLearning: An Introduction. Cambridge, MA, USA: A Bradford Book."
  }
]